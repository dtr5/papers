\documentclass[]{article}
\usepackage[letterpaper]{geometry}
\usepackage{amta2016}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{natbib}
\usepackage{layout}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}

\newcommand{\confname}{AMTA 2016}
\newcommand{\website}{\protect\url{http://www.amtaweb.org/}}
\newcommand{\contactname}{research track co-chair Lane Schwartz}
\newcommand{\contactemail}{lanes@illinois.edu} 
\newcommand{\conffilename}{amta2016}
\newcommand{\downloadsite}{\protect\url{http://www.amtaweb.org/}}
\newcommand{\paperlength}{$12$ (twelve)}
\newcommand{\shortpaperlength}{$6$ (six)}

%% do not add any other page- or text-size instruction here

\parskip=0.00in

\begin{document}

% \mtsummitHeader{x}{x}{xxx-xxx}{2016}{45-character paper description goes here}{Author(s) initials and last name go here}
\title{\bf Faster Neural Machine Translation Inference}  

\author{\name{\bf Hieu Hoang} \hfill  \addr{hieu@hoang.co.uk}\\ 
        \addr{}
\AND
       \name{\bf Tomasz Dwojak} \hfill \addr{???}\\
        \addr{Adam Mickiewicz University}
\AND
       \name{\bf Kenneth Heafield} \hfill \addr{???}\\
       %\name{\bf Kenneth Heafield} \hfill \addr{kheafiel@inf.ed.ac.uk}\\
        \addr{University of Edinburgh, Scotland}
}

\maketitle
\pagestyle{empty}

\begin{abstract}

Deep learning models are widely deployed for machine translation. In comparison on many other deep learning models, there are two characterisitics of machine translation which have not been adequately addressed by most software implementations, leading to slow inference speed. Firstly, as opposed to standard binary class models, machine translation models are used to discriminate between a large number of classes corresponding to the output vocabulary. Secondly, rather than a single label, the output from machine translation models is a sequence of class labels that makes up the words in the target sentence. The sentence lengths are unpredictable, leading to inefficiencies during batch processing. We provide solutions for these issues which together, increase batched inference speed by up to ??? on modern GPUs without affecting model quality. For applications where the use of maxi-batching introduces unacceptable delays in response time, our work speed up inference speed by up to ???. Our work is applicable to other language generation tasks and beyond.

\end{abstract}

\section{Introduction}

We will look at two areas that are critical to fast NMT inference where the models in NMT differ significantly from those in other applications. These areas have been overlooked by the general deep-learning community, we aim to improve their efficient for NMT-specific tasks.

Firstly, the number of classes in many deep-learning applications is small. However, the number of classes in NMT models is typically in the tens or hundreds of thousands, corresponding to the vocabulary size of the output language. For example, the best German-English NMT model at WMT14 contains 85,000 classes (Rico ???). This makes the output layer of NMT models very computationally expensive. ??? shows the breakdown of amount of time during translation using RNN model similar to (Rico ???); over 40\% of the time is involved in calculating the activation, softmax and the beam search. We will look at optimizations which explicitly target the output layer.

Secondly, mini-batching is often used to increase the speed of deep-learning applications, including NMT systems. (??? graph of batch v. speed) shows using batching can increase speed by 17 times. However, mini-batching does not take into acccount the variable length of NMT inputs and outputs, creating computational issues and efficiency challenges. The computational issues are often solved by masking unwanted calculations. Translation speed can often be improved by employing max-batching whereby the input set is pre-sorted by sentence length before mini-batching, creating mini-batches with similar length input sentences. However, the target sentence lengths will still differ even for similar length inputs but the standard mini-batchin algorithm must continue to process the batch until all target sentences have been completed. (??? batchsize size v. iterations) shows the number of sentences still being processed for each decoding iteration, for a specific batch. The number of sentences still to be processed decreases, reducing the effectiveness of mini-batching. We will propose an alternative to the mini-batching algorithm.

We based our model on the sequence-to-sequence model of (??? Cho) for machine translation models, but unlike (??? Devlin), we avoid solutions for the specific model. Therefore, our solution should be applicable to other models, architectures and task which have the similar characteristics. We envisage that our solutions would be of value to models used in text summarization, chatbot or image captioning. We also choose to focus on the use of GPU, rather than CPU as pursued in (??? Devlin).

\section{Prior Work}

Deep learning models have been successfully used on many machine learning task in recent years in areas as diverse as computer vision and natural language processing. This success have been followed by the rush to put these model into production. However, the computational resources required in order to run these models have ben chanllenging. One possible solution involves creating faster models that can approximate the original model (??? Student-Teacher model). Another solution is tackling specific computationally intensive functions by approximating them (??? softmax). 

Most current neural MT models follow an encoder-decoder architecture. The encoder calculates the word and sentence embeddings while the decoder generates the output sentence. The architecture within the encoder is still a subject of much research. (??? Kalshammer) used a convolution to encode the input and a recurrent neural network (RNN) for the decoder. RNNs was used for both encoding and decoding in (??? who used RNN 1st). (???) significantly improved translation quality by using LSTM was used in each RNN node. (??? Cho) used GRU that has the required properties of LSTM but is computationally cheaper. (??? Badhannau) added attention model which improved translation at a cost of slower speed.

There has been recent attempts to move away from the sequence-to-sequence models of RNN encoder-decoder. Some justify their architecture on faster inference as well as better translation results. (??? Attention is all you need) and (??? course-to-fine) has been proposed as a fast alternatives to RNN-based models as it is possible to process more of the units in parallel.

It has been noticed that half precision arithmetic can be used for deep learning model without significan loss of model quality (???). Other solutions include specialized hardware, the most popular being graphical processing units (GPU) but other hardware such as custom processors (??? TPU), FPGA (??? Intel) have been used. 

Many of these optimization are general purpose improvements for deep learning models, regardless of the task it is being used in. (??? Devlin) is a noticeable machhine translation-specific optimization which describe the speed improvements that can be obtained by techniques such as the use of half-precision, model changes and pre-computation.

\section{Proposal}
\label{sec:Proposal}

\subsection{Softmax and Beam Search Fusion}

The output layer of most deep learning models consist of the following steps
\begin{enumerate}
   \item \vspace{-2 mm} activation - multiplication of the input with the weight matrix for that layer
   \item \vspace{-2 mm} addition of a bias term to the resulting scores
   \item \vspace{-2 mm} activation function - in the output layers, this is commonly softmax or a variant, eg. log-softmax.
   \item \vspace{-2 mm} a search for the argmax output class, and probability is necessary.
\end{enumerate}

In models with a small number of classes such as binary classification, the computational effort required is trivial and fast. However, this is not the case for large number of classes such as those found in NMT models.

We shall leave the matrix multiplication for the next proposal and future work, and concentrate on the last three steps, the outline of which are shown in Figures~\ref{algo:Add Bias Term} to ~\ref{algo:Find best}.

\begin{figure} [h]
\begin{algorithmic}
\REQUIRE activation vector $p$, bias vector $b$
\FORALL{$p_i$ in $p$}
  \STATE $p_i \gets p_i + b_i$
\ENDFOR 
\end{algorithmic}
\caption{Add Bias Term}
\label{algo:Add Bias Term}
\end{figure}

\begin{figure} [h]
\begin{algorithmic}
\REQUIRE activation vector $p$

\COMMENT{calculate max for softmax stability}

\STATE $max \gets \infty$ 
\FORALL{$p_i$ in $p$}
  \IF{$p_i > max$}
    \STATE $max \gets p_i$
  \ENDIF
\ENDFOR 

\COMMENT{calculate denominator}

\STATE $sum \gets 0$ 
\FORALL{$p_i$ in $p$}
  \STATE $sum \gets sum + e^{p_i}$
\ENDFOR 

\COMMENT{calculate softmax}

\FORALL{$p_i$ in $p$}
  \STATE $softmax_i \gets \frac{e^{p_i}}{sum} $
\ENDFOR 

\RETURN $softmax$

\end{algorithmic}
\caption{Calculate softmax}
\label{algo:Calculate softmax}
\end{figure}

\begin{figure} [h]
\begin{algorithmic}
\WHILE{more input} 
  \STATE Create encoding batch
\ENDWHILE 
\end{algorithmic}
\caption{Find best}
\label{algo:Find best}
\end{figure}


As can be seen, the operations iterate over the matrix p five times - once to add the bias, three times to calculate the softmax, and once to search for the best class. We shall fused the three functions and combine the matrix access into two iterations, reducing memory bandwidth. Kernel fusion is a well practised optimization technique (???).

Secondly, we make use of the fact that softmax and exp are monotonic functions therefore we can avoid the search for the argmax in Figure~\ref{algo:Find best} as it will be the same as that in Figure~\ref{algo:Calculate softmax}.

Thirdly, we are only interested in the probability of the best class. Since we have precalculated the best class in Figure~\ref{algo:Calculate softmax}, we shall only calculate softmax for this one class, avoiding the computational and memory bandwith of computing the probability for all classes.

In fact, we are usually only interested in the argmax class during inference, not the probability. Therefore, by using the argmax calculated in Figure~\ref{algo:Calculate softmax}, we can avoid computing the softmax altogether. The outline of the our function is shown in Figure~\ref{algo:Fused Kernel}.

\begin{figure} [h]
\begin{algorithmic}
\WHILE{more input} 
  \STATE Create encoding batch
\ENDWHILE 
\end{algorithmic}
\caption{Fused Kernel}
\label{algo:Fused Kernel}
\end{figure}

The algorithm was extended from argmax to beam search for n-best classes.

\subsection{Top-up Batching}

The standard mini-batching algorithm is outlined in Figure~\ref{algo:Mini-batching}.

\begin{figure} [h]
\begin{algorithmic}
%\REQUIRE source sentence $s$, translation options
\WHILE{more input} 
  \STATE Create batch
  \STATE Encode
  \WHILE{batch is not empty} 
    \STATE Decode batch
    \FOR{each sentence in batch}
      \IF{translation is complete}
        \STATE Remove sentence from batch
      \ENDIF
    \ENDFOR
  \ENDWHILE
\ENDWHILE 
\end{algorithmic}
\caption{Mini-batching}
\label{algo:Mini-batching}
\end{figure}

This algorithm encode the sentences for a batch, followed by decoding the batch. The decoding stop once all sentences in the batch are completed. This is a potential inefficiency as the number of remaining sentences may not be optimal.

We will focus on decoding as this is the more compute-intensive step, and issues with differing sentence sizes in encoding can partly be ameriorated by maxi-batching.

Our proposed top-up batching algorithm encode and decode asynchronously. The encoding step, Figure~\ref{algo:Encoding for top-up batching}, is similar to the main loop of the standard algorithm but the results are added to a queue to be consumed by the decoding step later.

\begin{figure} [h]
\begin{algorithmic}
%\REQUIRE source sentence $s$, translation options
\WHILE{more input} 
  \STATE Create encoding batch
  \STATE Encode
  \STATE Add to queue
\ENDWHILE 
\end{algorithmic}
\caption{Encoding for top-up batching}
\label{algo:Encoding for top-up batching}
\end{figure}

Rather than decoding the same batch until all sentences in the batch are completed, the decoding step processing the same batch continuously. New sentences are added to the batch as old sentences completes, Figure~\ref{algo:Decoding for top-up batching}.

\begin{figure} [h]
\begin{algorithmic}
%\REQUIRE source sentence $s$, translation options
\STATE create decoding batch $b$ from queue
\WHILE{$b$ is not empty}
  \STATE Decode $b$
  \STATE Replace completed sentences with new sentence from queue
\ENDWHILE 
\end{algorithmic}
\caption{Decoding for top-up batching}
\label{algo:Decoding for top-up batching}
\end{figure}


\section{Experimental Setup}
\label{sec:Experimental Setup}


\section{Results}
\label{sec:Results}



\section{Conclusion}



 \section*{Acknowledgments}
This work is sponsored by the Air Force Research Laboratory, prime contract FA8650-11-C-6160.  The views and conclusions contained in this document are those of the authors and should not be interpreted as representative of the official policies, either expressed or implied, of the Air Force Research Laboratory or the U.S. Government.

\bibliographystyle{apalike}
\bibliography{amta2016,mt,more}


\end{document}
\grid
