\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2018}
\usepackage{times}
\usepackage{latexsym}

\usepackage[letterpaper]{geometry}
%\usepackage{amta2016}
\usepackage{url}
\usepackage{natbib}
\usepackage{layout}

\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx

\usepackage{graphicx}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\confname}{AMTA 2016}
\newcommand{\website}{\protect\url{http://www.amtaweb.org/}}
\newcommand{\contactname}{research track co-chair Lane Schwartz}
\newcommand{\contactemail}{lanes@illinois.edu} 
\newcommand{\conffilename}{amta2016}
\newcommand{\downloadsite}{\protect\url{http://www.amtaweb.org/}}
\newcommand{\paperlength}{$12$ (twelve)}
\newcommand{\shortpaperlength}{$6$ (six)}

%% do not add any other page- or text-size instruction here

\parskip=0.00in

\begin{document}

% \mtsummitHeader{x}{x}{xxx-xxx}{2016}{45-character paper description goes here}{Author(s) initials and last name go here}
\title{\bf Focusing on Fast Neural Network Inference}  

\author{\name{\bf Hieu Hoang} \hfill  \addr{hieu@hoang.co.uk}\\ 
        \addr{}
\AND
       \name{\bf Tomasz Dwojak} \hfill \addr{???}\\
        \addr{Adam Mickiewicz University}
\AND
       \name{\bf Kenneth Heafield} \hfill \addr{???}\\
       %\name{\bf Kenneth Heafield} \hfill \addr{kheafiel@inf.ed.ac.uk}\\
        \addr{University of Edinburgh, Scotland}
}

\maketitle
\pagestyle{empty}

\begin{abstract}

This paper describe the submissions to the efficiency track for GPUs by members of the University of Edinburgh, Adam Mickiewicz University, Tilde and University of Alicante. We focus on efficient implementation of the recurrent deep-learning model as implemented in Amun, the fast inference engine for neural machine translation. We improve the performance with an efficient mini-batching and maxi-batching algorithm. Translation speed was also reduced by fusing the softmax operation with the beam seach algorithm.


\end{abstract}

\section{Introduction}
\label{sec:Introduction}

As neural machine translation (NMT) models have become the new state-of-the-art, the challenge is to make their deployment efficient and economical. This is the challenge that this shared task is shining a spotlight on.

It is tempting to use a general purpose deep-learning toolkit such as Tensorflow or Pytorch to train and do the the inference where faster translation could be obtained by tuning parameters within the toolkit, and choosing fast models. %We believe this is the approach most other submissions have taken.

We take an opposing approach by using and enhancing a custom inference engine, Amun~\citep{junczys2016neural}, which we developed on the premise that fast deep-learning inference is an issue that deserves dedicated tools which are not compromised by other objectives such as training or support for multiple models. As well as delivering on the useful goal of fast inference, it can serve as a test-bed for novel ideas on neural network inference, and is useful as a means to explore the upper limit of the possible speed for a particular model and hardware. That is, Amun is an inference-only engine that supports a limited number of NMT models that put fast inference on modern GPU above all other considerations. %The consequence of this focused approach can be seen in the results of the shared task ???.

We submitted two systems to this year's shared task for the efficient translation on GPU. Our first submission was tailored to be as fast as possible while being above the baseline BLEU score. Our second submission trade some of the speed of first submission to return good quality translation.

We will describe below our experimental setup as well as the enhancements to Amun that we feel enabled it to achieve its outstanding speed.


\section{Improvements}

We desribe the main enhancements to Amun since the original 2016 publication.

\subsection{Batching}

The use of mini-batching is critical for fast model inference. The size of the batch is determined by the number of inputs sentences to the encoder in an encoder-decoder model. However, the number of batches during decoding can vary as some sentences have completed translating or the beam search add more hypotheses to the batch.

It is tempting to ignore these considerations, for example, by always decoding with a constant batch and beam size and ignoring hypotheses which are not needed but this comes at a cost of lower translation speed due to wasteful processing.

The Amun engine implements an efficient batching algorithm that takes into account the actual number of hypotheses that needs to be decoded at each decoding step, Figure~\ref{algo:Mini-batching}.

\begin{algorithm}
\begin{algorithmic}
\Procedure{Batching}{encoded sentences $i$}

\State Create batch $b$ from $i$

\While{$b \neq \emptyset$} 
  \State $b' \gets$ DecodeAndBeamSearch($b$)
  \State $b \gets \emptyset$
  
  \ForAll{hypo $h$ in $b'$}
    \If{$h \neq \textless/s \textgreater$  }
      \State Add $h$ to $b$
    \EndIf
  \EndFor
\EndWhile

\EndProcedure

\end{algorithmic}
\caption{Mini-batching}
\label{algo:Mini-batching}
\end{algorithm}


\subsection{Softmax and Beam Search Fusion}

Most NMT more predict a large number of classes in their output layer, corresponding to the number of words or subword units in their target language. For example, ~\citet{sennrich-haddow-birch:2016:P16-12} experimented with target vocabulary sizes of 60,000 and 90,000 sub-word units. This makes the output layer of NMT models very computationally expensive. Figure~\ref{fig:pie-time-europarl} shows the breakdown of amount of time during translation our NMT system; nearly 70\% of the time is involved in the output layer. We describe the steps to fuse the operations to improve efficiency.
\begin{figure}
\centering
\begin{tabular}{cc}
{\includegraphics[scale=0.3]{pie-time.png}} 
\end{tabular}
\caption{Proportion of time spent during translation}
\label{fig:pie-time.png}
\end{figure} 

The output layer of most deep learning models consist of the following steps
\begin{enumerate}
   \item \vspace{-2 mm} multiplication of the weight matrix with the input vector $p = w x$
   \item \vspace{-2 mm} addition of a bias term to the resulting scores $p = p + b$
   \item \vspace{-2 mm} applying the activation function, most commonly softmax $ p_i = \exp(p_i) / \sum \exp(p_i) $
   \item \vspace{-2 mm} a beam search for the best (or n-best) output classes $\argmax_i p_i$
\end{enumerate}

In models with a small number of classes such as binary classification, the computational effort required is trivial and fast but this is not the case for the large number of classes found in typical NMT models.

We leave step 1 for future work and focus on the last three steps, the outline for which are shown in Algorithm~\ref{algo:original-softmax-beamsearch}. For brevity, we show the algorithm for 1-best, a beam search of the n-bests is a simple extension of this.

\begin{algorithm} 
\begin{algorithmic}

\Procedure{AddBias}{vector $p$, bias vector $b$}
\ForAll{$p_i$ in $p$}
  \State $p_i \gets p_i + b_i$
\EndFor 
\EndProcedure

\State

\Procedure{Softmax}{vector $p$}

\Comment{calculate max for softmax stability}
\State $max \gets - \infty$ 
\ForAll{$p_i$ in $p$}
  \If{$p_i > max$}
    \State $max \gets p_i$
  \EndIf
\EndFor

\Comment{calculate denominator} 
\State $sum \gets 0$ 
\ForAll{$p_i$ in $p$}
  \State $sum \gets sum + \exp(p_i - max)$
\EndFor

\Comment{calculate softmax}
\ForAll{$p_i$ in $p$}
  \State $p_i \gets \frac{\exp(p_i - max)}{sum} $
\EndFor 

\EndProcedure

\State

\Procedure{Find-Best}{vector $p$}

\State $max \gets - \infty$ 
\ForAll{$p_i$ in $p$}
  \If{$p_i > max$}
    \State $max \gets p_i$
    \State $best \gets i$
  \EndIf
\EndFor 

\Return $max$, $best$

\EndProcedure

\end{algorithmic}
\caption{Original softmax and beam Search Algorithm}
\label{algo:original-softmax-beamsearch}
\end{algorithm}


As can be seen, the matrix p is iterated over five times - once to add the bias, three times to calculate the softmax, and once to search for the best classes. We propose fusing the three functions into one kernel, a popular optimization technique~\citep{Guevara2009EnablingTP}, making use of the following observations.

Firstly, softmax and $\exp$ are monotonic functions, therefore, we can move the search for the best class from FIND-BEST to SOFTMAX, at the start of the kernel.

Secondly, we are only interested in the probabilities of the best classes. Since they are now known at the start of the kernel, we compute softmax only for those classes.

\begin{algorithm}
\begin{algorithmic}
\Procedure{Fused-Kernel}{vector $p$, bias vector $b$}

\Comment{add bias, calculate $max$ \& $argmax$}

\State $max \gets - \infty$ 
\State $sum \gets 0$ 

\ForAll{$p_i$ in $p$}
  \State $p_i' \gets p_i + b_i$  
  \If{$p_i' > max$}
    \State $\Delta \gets max - p_i'$
    \State $sum \gets \Delta \times sum + 1 $
    \State $max \gets p_i'$
    \State $best \gets i$
  \Else
    \State $sum \gets sum + \exp(p_i' - max)$
  \EndIf
\EndFor

\Return $\frac{1}{sum}$, $best$ 

\EndProcedure
\end{algorithmic}

\caption{Fused softmax and beam search}
\label{algo:Fused Kernel}
\end{algorithm}

Thirdly, the calculation of $max$ and $sum$ can be accomplished in one loop by adjusting $sum$ whenever a higher $max$ is found during the looping:
\begin{eqnarray*}
sum & = & e^{x_t - max_b} + \sum_{i=0...t-1}{e^{x_i - max_b}} \\
    & = & e^{x_t - max_b} + \sum_{i=0...t-1}{e^{x_i - max_a + \Delta}}  \\
    & = & e^{x_t - max_b} + e^{\Delta} \times \sum_{i=0...t-1}{e^{x_i - max_a}} 
\end{eqnarray*}
where $max_a$ is the original maximum value, $max_b$ is the new, higher maximum value, ie. $max_b > max_a$, and $\Delta = max_a - max_b$. The outline of our function is shown in Algorithm~\ref{algo:Fused Kernel}.

In fact, a well known optimization is to skip softmax altogether and calculate the argmax over the input vector, Algorithm~\ref{algo:Argmax only}. This is only possible for beam size 1 and when we are not interested in returning the softmax probabilities.

\begin{algorithm}
\begin{algorithmic}
\Procedure{Fused-kernel-1-best}{vector $p$, bias vector $b$}

\State $max \gets - \infty$ 
\ForAll{$p_i$ in $p$}
  \If{$p_i + b_i > max$}
    \State $max \gets p_i + b_i$
    \State $best \gets i$
  \EndIf
\EndFor

\Return $best$ 

\EndProcedure

\end{algorithmic}
\caption{Find 1-best only}
\label{algo:Argmax only}
\end{algorithm}

%These changes do not reduce the asymptotic runtime of our algorithm, which remains at $\mathcal{O}(|p|)$, but it does bring the number of iterations over $p$ down from 5 to 1. In practise, this has a significant affect on inference speed.

\subsection{Half-Precision}

Reducing the number of bits needed to store floating point values from 32-bits to 16-bits promises to increase translation speed through faster calculations and reduced bandwidth usage. 16-bit floating point operations are supported by the GPU hardware and software available in the shared task.

In practise, however, efficiently using half-precision value requires a comprehensive redvelopment of the GPU code. We therefore make do with using the GPU's Tensor Core\footnote{https://devblogs.nvidia.com/programming-tensor-cores-cuda-9/} fast matrix multiplication routines which transparently converts 32-bit float point input matrices to 16-bit values and output a 32-bit float point product of the inputs.

\section{Experimental Setup}
\label{sec:Experimental Setup}

Both of our submitted systems uses the sequence-to-sequence model similar to that described in ~\citet{sennrich-haddow-birch:2016:P16-12}, containing a bidirectional RNN in the encoder and a two-layer RNN in the decoder. We use BPE to adjust the vocabulary size.

We used a variety of GPUs to train the models but all testing was done on an Nvidia V100. Translation quality was measured using BLEU, specifically multi-bleu as found in the Moses toolkit\footnote{https://github.com/moses-smt/mosesdecoder}. The validation and test sets provided by the shared task organisers were used to measure translation quality, but a 50,000 sentence subset of the training data was used to measure translation speed to obtain longer, more accurate measurements.

\subsection{GRU-based system}

Our first system uses gated recurrent units (GRU) throughout, trained with Marian~\citep{junczys2018marian}, but submitted both systems using the Amun inference engine.

We experimented with varying the vocabulary size and the RNN state size before settling for a vocabulary size of 30,000 (for both source and target language) and 256 for the state size, Table~\ref{tab:BLEU for newstest2014}.

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|r|} \hline
		& \multicolumn{3}{|c|}{\emph{State dim}}	\\ \hline	
Vocab size	& 256	& 512	& 1024 \\ \hline
1.000 		& 12.23 &	& 12.77 \\ 
5,000		& 16.79	&  	& 17.16 \\ 
10,000		& 18.00	& 	& 18.19 \\
20,000		& -	&	& 19.52 \\ 
30,000		& 18.51	& 19.17	& 19.64 \\ \hline
\end{tabular}
\end{center}
\caption{Validation set BLEU (newstest2014) for GRU-based model}
\label{tab:BLEU for newstest2014}
\end{table}

After further experimentation, we decided to use sentence length normalization and NVidia's Tensor Core matrix multiplication which increased translation quality as well as translation speed. The beam was kept at 1 throughout for the fastest possible inference.

\subsection{mLSTM-based system}

Our second system uses multiplicative-LSTM in the encoder and the first layer of a decder, and a GRU in the second layer, using an extension of the Nematus toolkit which supports such model. We trained 2 systems with differing vocabulary sizes and varied the beam sizes before choosing the the settings that produces good translation quality on the validation set, Table~\ref{tab:mLSTM BLEU - vocab sizes}. 

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|} \hline
		& \multicolumn{2}{|l|}{\emph{Vocab size}}	\\ \hline	
Beam size	& 40,000	& 50,000 \\ \hline
1 		& 23.45		& 23.32	\\ 
2		& 24.15		& 24.04	\\
3		& 24.48		&  	\\
4		& 24.42		& 	\\
5		& 24.48		& 	\\ \hline
\end{tabular}
\end{center}
\caption{Validation set BLEU for mLSTM-based model}
\label{tab:mLSTM BLEU - vocab sizes}
\end{table}

\section{Result}
\label{sec:Result}

\subsection{Batching}

The efficiency of Amun's batching algorithm can be seen by observing the time taken for each decoding step in a batch of sentence, Figure~\ref{fig:decode-step}. Amun's decoding becomes faster as sentences finish translating. This contrast with the Marian inference engine, which uses a simpler batching algorithm, where the speed stays relatively constant throughout the decoding of the batch.

\begin{figure}
\centering
\begin{tabular}{cc}
{\includegraphics[scale=0.3]{decode-step.png}} 
\end{tabular}
\caption{Time taken for each decoding step for a batch of 1280 sentences}
\label{fig:decode-step}
\end{figure} 

Using batching can increase the translation speed by over 20 times in Amun and shows little degradation with larger batch sizes, Figure~\ref{fig:batch-size}. Just as importantly, however, it doesn't suffer degradation with large batch sizes unlike the simpler algorithm which slows down when the batch size is above 1000. This scalability issue is likely to increase with ever increasing core counts.


\begin{figure}
\centering
\begin{tabular}{cc}
{\includegraphics[scale=0.3]{batch-size.png}} 
\end{tabular}
\caption{Speed v. batch size}
\label{fig:batch-size}
\end{figure} 


\subsection{Softmax and Beam Search Fusion}

Fusing the bias and softmax operations in the output layer with the beam search results in a speed improvement by 25\%, Figure~\ref{fig:fused}. Its relative improvement decrease marginally with increasing beam size.

\begin{figure}
\centering
\begin{tabular}{cc}
{\includegraphics[scale=0.3]{fused.png}} 
\end{tabular}
\caption{Using fused operation}
\label{fig:fused}
\end{figure} 

Further insight can be gained by examining the time taken for each step in the output layer and beam search, Table~\ref{tab:fused-breakdown}. The fused operation only has to loop through the large cost matrix once, therefore, for low beam sizes its is comparable in speed to the simple kernel to add the bias. For higher beam sizes, the cost of maintaining the n-best list is felt.

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|} \hline
		& Baseline	& Fused \\ \hline
\multicolumn{3}{|c|}{\emph{Beam size 1}}	\\ \hline	
Multiplication 	& 5.39 		& 5.38 (+0\%) \\ \hline
Add bias 	& 1.26		&  \\ 
Softmax 	& 1.69		& 2.07 (-86.6\%)\\
Beam search	& 12.53		&  \\ \hline
\multicolumn{3}{|c|}{\emph{Beam size 3}}	\\ \hline	
Multiplication 	& 14.18 	& 14.16 (+0\%) \\ \hline
Add bias 	& 3.76		&  \\ 
Softmax 	& 4.75		& 3.43 (-87.1\%)\\
Beam search	& 18.23		&  \\ \hline
\multicolumn{3}{|c|}{\emph{Beam size 9}}	\\ \hline	
Multiplication 	& 38.35 	& 38.42 (+0\%) \\ \hline
Add bias 	& 11.64		&  \\ 
Softmax 	& 14.4		& 17.5 (-72.1\%)\\
Beam search	& 36.7		&  \\ \hline
\end{tabular}
\end{center}
\caption{Time taken (sec) breakdown}
\label{tab:fused-breakdown}
\end{table}

\subsection{Tensor Cores}

By taking advantage of the GPU's hardware accelerated matrix multiplication, we can gain up to 20\% in speed, Table~\ref{tab:tensor-cores}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{|l|r|r|} \hline
Beam size	& Baseline	& Tensor Cores \\ \hline
1	 	& 39.97 	& 34.54 (-13.6\%) \\ 
9 		& 145.8		& 116.8 (-20.0\%) \\ \hline
\end{tabular}
\end{center}
\caption{Time taken (sec) using Tensor Cores}
\label{tab:tensor-cores}
\end{table}


\section{Conclusion and Future Work}

We have presented some of the improvement to Amun which are focused to improvement NMT inference.

We are also working to make deep-learning faster using more specialised hardware such as FPGAs. It would be interesting as future work to bring our focused approach to fast deep-learning inference to a more general toolkit. 

%\section*{Acknowledgments}
%This work is sponsored by the Air Force Research Laboratory, prime contract FA8650-11-C-6160.  The views and conclusions contained in this document are those of the authors and should not be interpreted as representative of the official policies, either expressed or implied, of the Air Force Research Laboratory or the U.S. Government.

\bibliographystyle{apalike}
\bibliography{amta2016,mt,more}


\end{document}
\grid
\grid
