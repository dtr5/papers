%% Sample file $Id: PBML_article.tex 134 2010-07-01 19:28:04Z popel $
\documentclass{pbml}
%\documentclass[nofonts]{pbml} % for XeLaTeX without Pagella and DejaVu fonts
%\documentclass[color]{pbml} % for color images and hypertext links

% This is a sample file for the PBML article.
% You can compile it with (ordered by preference)
%  1. XeLaTeX with installed fonts TeX Gyre Pagella and DejaVu
%  2. XeLaTeX without the fonts -> you must use \documentclass[nofonts]{pbml}
%  3. pdfLaTeX
%
% In all three methods, you can use Unicode (utf8) encoding for special letters
% (so instead of na\"{i}ve you can write directly naïve).
% Note that with methods 2 and 3, your output will be slightly DIFFERENT than
% our final print version (different line breaks and page breaks).
% See the end of this file for more information.

% Packages
% ========
% In this place you can load required packages by the \usepackage commands.
% The following packages are loaded automatically by the class:
% euler    (for math fonts)
% graphicx (for inclusion of images)
% multicol (for multicolumn typesetting)
% natbib   (for bibliography citations)
% amssymb  (for various symbols)
% If XeLaTeX is used, fontspec and xltxtra are loaded as well.

% Definitions
% ===========
% You can use your own macros defined by \newcommand, \providecommand,
% \DeclareRobustCommand (or even by \def) as well as environments declared by
% \newenvironment. You can also use \newcounter in order to declare your own counters.

\begin{document}

% Document title and authors
% ==========================
% Due to journal organization, the article title and authors must be specified
% AFTER \begin{document}. Note that \subtitle is not allowed anymore due to
% problems with indexing in science databases. If needed use colon in the title.

\title{A Comparison of Synchronous-CFG \titlelinebreak{} Decoder Implementations}


% Now put the affiliated institutes first, then author names and the labels
% of their institutes in the "institute" field. The order of institutes and
% authors printed below the title will be the same as the order of your commands.
% Each author can be associated with more than one institute.
% One author must be chosen as the "corresponding author" (using attribute
% corresponding) and his or her email and full address must be provided.
% PLEASE, use Unicode (utf8) encoding (e.g ï instead of \"{i}).

\institute{label1}{University of Edinburgh}
\institute{label2}{Center for Language and Speech Processing, Johns Hopkins University}
%\institute{label3}{Carnegie Mellon University}

\author{
  firstname=Hieu,
  surname=Hoang,
  institute=label1,
}
\author{
  firstname=Matt,
  surname=Post,
  institute=label2,
}
\author{
  firstname=Kenneth,
  surname=Heafield,
  institute=label1,
}
%\author{
%  firstname=Chris,
%  surname=Dyer,
%  institute=label3,
%}
\author{
  firstname=Eva,
  surname=Hasler,
  institute=label1,
}
\author{
  firstname=Phil,
  surname=Williams,
  institute=label1,
}
\author{
  firstname=Barry,
  surname=Haddow,
  institute=label1,
}
\author{
  firstname=Philipp,
  surname=Koehn,
  institute=label1,
}

% If all authors belong to the same institute, you can use simpler syntax:
% \institute{}{Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics}
% \author{firstname=Humpty, surname=Dumpty}
% \author{firstname=Mock, surname=Turtle,
%   corresponding=yes,
%   email={turtle@seacoast.wl},
%   address={Institute of Formal and Applied Linguistics\\
%            Faculty of Mathematics and Physics,\\
%            Charles University in Prague\\
%            Malostranské náměstí 25\\
%            118 00 Praha 1, Czech Republic}}
% \author{firstname=Cheshire, surname=Cat}


% The title and authors' names are used in the running head. If they are
% long, you should define short versions. These definitions are optional. You
% define them only if they are needed. The example follows:
\shorttitle{SCFG Decoder Implementations}
\shortauthor{H. Dumpty, M. Turtle, C. Cat}

% Now print the title by:
\PBMLmaketitle


% Abstract
% ========
% The abstract is placed within the "abstract" environment. It is a mandatory
% part of the article. PLEASE, do not use your own macros in abstract, if possible.

\begin{abstract}
Best practise for developing a SMT decoder has progressed enormously since the initial release of the Moses decoder in 2006. The emergence of syntax-inspired models, sparse features and more features, have been the driver for the major changes in the framework of the decoder.

In this paper, we describe the changes in the Moses decoder to support such changes. How we have updated the decoder, and dependent programs, while preserving the rich features in the toolkit. Particular emphasis will be place in describing the feature function framework.

The open-source community has also changed from the dominance of phrase-based model and virtual Moses monopoly, to a situation where syntactic models are leading the research agenda, and other projects such as cdec and Joshua have equal prominence.
We take this opportunity to compare the implementations of these other decoders with Moses. We find that there are many similarities between the decoders, but also some surprising differences in implementation of even very fundamental features in each decoder.

\end{abstract}

\section{Introduction}
% The body of the article
% =======================
% The PBML class is modelled after the standard article class. This means
% that you can use almost everything that is allowed in articles as described
% in the textbooks of LaTeX. We support sectioning commands \section,
% \subsection and \subsubsection.

% In addition to the \cite command, you can use natbib style of citations.
% PLEASE, use \citet instead of \cite if the author names are part of the sentence.
%\citet{PDT2} show \ldots \\ % This renders as "Hajič et al. (2006) show" instead of "(Hajič et al., 2006) show".
%\ldots tree-based annotation (e.g.~\citealp{PDT2}).

In terms of extensibility, the Moses\cite{koehn-EtAl:2007:PosterDemo} decoder was a vast improvement over its immediate predecessor, Pharoah\cite{Koehn:Pharaoh:2004}. 

Moses allows multiple translation models and language models instances and implementations. Allowing multiple instances has enabled researchers to combine multiple datasets in novel ways, as well as the use of secondary models on linguistic information\cite{Koehn:Philipp:2007, Sennrich:Rico:2011,Koehn:Philipp:2012}. Allowing multiple implementations has spurred on the developments of smaller, faster, better implementation of language models and phrase tables\cite{talbot-osborne:2007:ACLMain, Heafield-kenlm, Junczys-Dowmunt:Marcin:2012}.

However, a direction of machine translation research in the last few years has been improving translation quality with the addition of ad-hoc feature functions outside of the translation model and language model types. Also, the use of sparse features during decoding has also been popular. Research such as \cite{chiang-knight-wang:2009:NAACLHLT09, cherry:2013:NAACL-HLT} typifies this approach.

Enabling ad-hoc features and sparse features requires a more extensible framework that incorporate feature functions into the decoder.

It also required tuning algorithms which are able to deal with a large number of features. It is generally known that MERT is unstable with a large number of features.

This paper describes this new feature function framework as implemented in the Moses decoder, and compares it with that implemented in cdec\cite{Dyer_etal_2010} and Joshua\cite{Joshua-Decoder}.

\section{Feature Function Framework}
\subsection{Old Framework}
The built-in feature functions in the phrase-based models are:

i. Phrase-table

ii. Language model

iii. Distortion model

iii. Lexicalized Reordering model

    iv. Word penalty

    v. Unknown word penalty

The hierarchical and syntax model also uses these models, except for the distortion and lexicalized reordering model.

Each feature function was implemented in a separate class, with no common class hierarchy. The initialisation and evaluation of each feature function was entirely bespoked.

There is also no centralized store of feature functions. A new feature function requires a new variable to store the object.

Feature function state information is an essential component of the search procedure in decoding. State information control hypothesis recombination, enabling the decoder to effectively search through many more hypotheses than it actually has. State information is emitted from stateful feature functions.

The state information in Moses has evolved since inception. Initially, state information was only available from language models, therefore, only language models states were compare, Figure (1).

    CompareHypotheses(a, b)

       if Coverage(a) != Coverage(b)

         return not equal

        for each language model state s

          if (state type s for hypothesis a) != (state type s for hypothesis b)

             return not equal

        end-for
        return equal

\subsection{New Framework}        
 All feature functions inherit indirectly from the class

    FeatureFunction

which provides basic services to all feature functions such as storing the name and number of dense features, and providing a central repository of all feature functions.

A feature function can be stateless or stateful, depending on whether it inherits from

StatelessFeatureFunction

StatefulFeatureFunction

The state comparison routine of Figure (1) evolved to a more general comparison of states from all stateful feature functions, Figure (2).

    CompareHypotheses(a, b)

       if Coverage(a) != Coverage(b)

         return not equal

        for each feature function state s

          if (state type s for hypothesis a) != (state type s for hypothesis b)

             return not equal

        end-for
        return equal
        
\subsection{CDEC and Joshua}
CDEC and Joshua both have modern feature function frameworks similar to the Moses new framework. CDEC has no distinction between stateful and stateless features; stateless features have zero size states.

\section{Scoring}
\subsection{Old Framework}
The types of feature functions were hardcoded when evaluating the scores of translation rules and hypotheses. For example, a translation rule is scored according to Equation (1)

\begin{equation}
  score = \sum_{i \in T}(\lambda_i * score_i) + \sum_{i  \in L} (\lambda_i * score_i) + \lambda_{wp} * words
\end{equation}

where $T$ is the set of translation models, and $L$ is the set of sequence models.

Similarly, when evaluating a hypothesis, the score is calculated as follows:

\begin{equation}
 score =  \sum_{i \in T} (\lambda_i * score_i)
    + \sum_{i \in L} (\lambda_i * score_i)
    + \lambda_{wp} * words
    + \lambda_d * score_d
    + \sum_{i \in R}(\lambda_i * score_i)
    + score_{future}
\end{equation}

where $\lambda_d$ and $score_d$ is the distortion weight and score respectively, $R$ is the set of lexicalized reordering models.

When adding a new feature function, it would not be included in the scoring of the translation rule and hypotheses until Equation (1) and (2) are update.

Translation rule always evaluated occurs before decoding, either during loading of the feature function, or just before the translation rule is used in decoding.

Hypotheses are evaluated when they are created during the search process.

\subsection{New Framework}
The new framework better supports the creation of arbitrary feature functions by generalizing all code that deals with feature functions.

Equation (1) becomes Equation (...)
\begin{equation}
  score = \sum_{i \in FF}(\lambda_i * score_i)
\end{equation}
where $FF$ is the set of feature functions.

Equation (2) becomes Equation (...)
\begin{equation}
  score =  \sum_{i \in FF}(\lambda_i * score_i)
	  + score_{future}
\end{equation}

Furthermore, there are three places where each feature function can be evaluated. At each place, a different range of information is available to the feature function to access for information.

    1. During creation of translation rules. Access to the words in the source and target side of translation rule.

    2. Just before decoding. Access to the entire source sentence to be decoded.

    3. During decoding. Access to the context information is available, containing the hypotheses create to translate the sentence so far. The exact information contained in the hypotheses is dependent on whether the formalism used.

Scores that are evaluated at stage one can be cached and reused inter-sentence. Scores evaluted during stage (2) and (3) cannot be cached as they depend on the entire source sentence. Therefore, it is more efficient to evaluate as early as possible.

Each feature function can override any or all of the method to evaluate at each of these stage. For example. a simple word penalty feature would typically implement its evaluation in stage (1) as access to the translation rule is sufficient information to allow it to evaluate the word penalty score.

On the other hand, a feature that depends on the bag of words of the input sentence would override the method for stage (2).

Stateful features, and features that depend on the segmentation of the input sentence, need to override stage (3).

The most efficient way to implement language models is to override stage (1) and (3). As a stateful feature, it must override stage (3) to access adjacent target words to score overlapping n-grams. However, it is able to score the n-grams within each translation rule, and also calculate the estimated future cost, without the context. By evaluating the stateless portion of the language model score during stage (1) mitigate these score from having to be re-evaluated each time the translation rule is applied. This behaviour codify the existing practises by building the framework to support it, alllowing other feature function to take advantage of this efficient mechanism.

Additionally, some feature functions do not override any of the stages. These feature is so intertwined with the search operation that their scores are set by search operation themselves. The feature function exist as an identifier to locate the score. Features functions such as the translation models, unknown word penalty, and generation models score fall into this category.

\subsection{CDEC and Joshua}
Moses implements a one-pass search strategy. All feature functions are evaluated when a hypothesis is created.

Joshua and CDEC employs a multi-pass search strategy to translate each sentence. Each pass creates a hypergraph that is used as the input for the next pass. Both decoders evaluates stateless feature functions in the first pass, then stateful feature functions in the second.

CDEC also supports an arbitary number of passes and split the feature functions between those passes.

\section{File Format}
\subsection{Old Framework}
Each feature function also required a separate section in the configuration (moses.ini) file to be created, along with the code to read the new section. For example, translation models are specified in the section

  [ttable-file]

Language models in

  [lmodel-file]

Furthermore, feature function also need a separate weight section, for example, for translation models:

  [weight-t]

and language models:

 [weight-l]

New feature functions need to create and manage their own specification and sections.

The features that can be tuned by MERT is also hardcoded in the tuning script. The MERT script has to be altered every time a new feature function is integrated into Moses.

\subsection{New Framework}
The configuration file format has been changed to minimize the amount of boilerplate code necessary when adding new feature functions. All feature functions are declared in the configuration file in the section

  [feature]

In the fixed format

  Feature-Name [key=value]*

For example,

  KENLM factor=0 path=lm.arpa order=5

All dense weights are put into the section

  [weight]

In the fixed format

  Feature-Instance= [value]*

Similarly, weights for sparse features are in separate file referenced by the section

  [weight-file]

in the format
  Feature-Instance\_feature value
  
\section{Sparse Feature Function}

Sparse features contain scores that may only be present occasionally. Also, there may be many more sparse features than dense features. There are typically 12 scores in a standard phrase-based model and 8 scores in a hierarchical phrase-based model. There may be millions of sparse score in a system with sparse features. However, only a few will be present in a particular hypothesis.

This presents efficiency challenges which each decoder has attempted to solve in a different way.

\subsection{Moses}
Moses explicitly implements both dense and sparse features, and each score must be identifiable as one or the other.
The intuition behind this is that most systems will still contain dense features. Therefore, it would be more efficient to use dedicated dense vectors for these scores.

A sparse feature map was added to handle sparse scores.

\subsection{CDEC}
CDEC eschew the dense and sparse hybrid approach and implement only sparse maps. Dense features can still be modelled in the framework, with some loss of efficiency.

To improve speed and memory usage, hypotheses only store deltas of sparse features differences with antecedent hypotheses.

\subsection{Joshua}
As with CDEC, Joshua only supports sparse features, however, it uses a different tack to improve efficiency.

Hypotheses only retain the weighted sum of the feature scores. They do not retain the individual score components unless required, for example, in the n-best list. The intuition behind this approach is that the score breakdown is only required in some situations such as producing n-best lists for tuning. In most other circumstances, they can be traded for better performance.
  
% For figures and tables always use "figure" resp. "table" floating environments
% and always supply a caption. See the paragraph about color images below.
%\begin{figure}
% \begin{center}
%  \includegraphics[width=\textwidth]{example_figure}
%  \caption{Example figure}\label{fig:example}
% \end{center}
%\end{figure}

\section*{Acknowledgements}
This research is supported by \ldots

% Bibliography
% ============
% You may either enter the bibliography manually, possibly making use of
% \label and \ref, or use BibTeX. The bibliography style is set
% automatically. You process the bibliography by BibTeX in the
% standard way and include it by:
\bibliography{mybib}

% If needed, add appendices here
%\section*{Appendix A: \ldots}

\correspondingaddress
\end{document}

% ======= Additional information ==========

% XeLaTeX
% =======
% The Prague Bulletin of Mathematical Linguistics is typeset by XeLaTeX.
% It is not required that you use XeLaTeX and the same fonts as will be used in the
% journal but it is better to do so if you could (so you have the same line and page breaks).
% If XeLaTeX is not available on your computer, you can even used standard LaTeX.

% Fonts
% =====
% In the printed version, fonts TeX Gyre Pagella (shipped with TeX Live 2008 or later)
% and DejaVu (http://dejavu.sourceforge.net/wiki/index.php/Main_Page)
% will be used. If you have XeLaTeX but not these fonts, you may need to install
% the fonts somewhere where your system (fontconfig) can find them. On Linux,
% try something like this:
% ln -s YOUR-TEXLIVE/2008/texmf-dist/fonts/opentype/public/ \
%  ${HOME}/.fonts/texlive-2008-otf-public
% If you are not able to install these fonts, you can instruct XeLaTeX
% to use its default fonts by \document[nofonts]{pbml}.

% If you want to typset examples in east Asian scripts, you have to use
% OpenType Unicode fonts that are freely redistributable and you have to
% include them with your article. If you must use nonfree or non-Unicode
% fonts, you must supply the examples as EPS or PDF with fonts embedded in
% the files (as the required subset).
% If encountering problems with the "bidi" package, contact pbml@ufal.mff.cuni.cz.

% Vector Graphics
% ===============
% The most portable graphics package is tikz which is a front end to pgf.
% This package is fully supported both in standard LaTeX and XeLaTeX.
% PSTricks is not currently available for XeLaTeX.

% Color Images
% ============
% The printed version of PBML does not allow color figures, so please convert
% all your figures to grayscale (or black&white), make sure they are legible
% and put them in a subdirectory called "grayscale".
% When specifying the filename in \includegraphics DO NOT include the
% subdirectory.
% Optionally, in addition to the default grayscale images, you can add also
% color versions of the figures to be published in the online version.
% Put such images in a subdirectory called "color" with the same filenames
% as the grayscale ones.
% To temporarily switch to color output and hypertext links, use
% \documentclass[color]{pbml}

% Including images and other files
% ===================================
% When loading the file you must use a relative path, never the full path.
% Load the images by \includegraphics
% and other files by \input, never use \include.
