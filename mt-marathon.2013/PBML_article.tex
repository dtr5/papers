%% Sample file $Id: PBML_article.tex 134 2010-07-01 19:28:04Z popel $
\documentclass{pbml}
%\documentclass[nofonts]{pbml} % for XeLaTeX without Pagella and DejaVu fonts
%\documentclass[color]{pbml} % for color images and hypertext links
% test

% This is a sample file for the PBML article.
% You can compile it with (ordered by preference)
%  1. XeLaTeX with installed fonts TeX Gyre Pagella and DejaVu
%  2. XeLaTeX without the fonts -> you must use \documentclass[nofonts]{pbml}
%  3. pdfLaTeX
%
% In all three methods, you can use Unicode (utf8) encoding for special letters
% (so instead of na\"{i}ve you can write directly naïve).
% Note that with methods 2 and 3, your output will be slightly DIFFERENT than
% our final print version (different line breaks and page breaks).
% See the end of this file for more information.

% Packages
% ========
% In this place you can load required packages by the \usepackage commands.
% The following packages are loaded automatically by the class:
% euler    (for math fonts)
% graphicx (for inclusion of images)
% multicol (for multicolumn typesetting)
% natbib   (for bibliography citations)
% amssymb  (for various symbols)
% If XeLaTeX is used, fontspec and xltxtra are loaded as well.
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{tikz,gnuplot-lua-tikz} 

% Definitions
% ===========
% You can use your own macros defined by \newcommand, \providecommand,
% \DeclareRobustCommand (or even by \def) as well as environments declared by
% \newenvironment. You can also use \newcounter in order to declare your own counters.

\begin{document}

% Document title and authors
% ==========================
% Due to journal organization, the article title and authors must be specified
% AFTER \begin{document}. Note that \subtitle is not allowed anymore due to
% problems with indexing in science databases. If needed use colon in the title.

\title{A Comparison of Synchronous-CFG \titlelinebreak{} Decoder Implementations}


% Now put the affiliated institutes first, then author names and the labels
% of their institutes in the "institute" field. The order of institutes and
% authors printed below the title will be the same as the order of your commands.
% Each author can be associated with more than one institute.
% One author must be chosen as the "corresponding author" (using attribute
% corresponding) and his or her email and full address must be provided.
% PLEASE, use Unicode (utf8) encoding (e.g ï instead of \"{i}).

\institute{label1}{University of Edinburgh}
\institute{label2}{Center for Language and Speech Processing, Johns Hopkins University}
\institute{label3}{Carnegie Mellon University}

\author{
  firstname=Hieu,
  surname=Hoang,
  institute=label1,
}
\author{
  firstname=Kenneth,
  surname=Heafield,
  institute=label1,
}
\author{
  firstname=Matt,
  surname=Post,
  institute=label2,
}
\author{
  firstname=Eva,
  surname=Hasler,
  institute=label1,
}
\author{
  firstname=Phil,
  surname=Williams,
  institute=label1,
}
\author{
  firstname=Barry,
  surname=Haddow,
  institute=label1,
}
\author{
  firstname=Chris,
  surname=Dyer,
  institute=label3,
}
\author{
  firstname=Philipp,
  surname=Koehn,
  institute=label1,
}

% If all authors belong to the same institute, you can use simpler syntax:
% \institute{}{Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics}
% \author{firstname=Humpty, surname=Dumpty}
% \author{firstname=Mock, surname=Turtle,
%   corresponding=yes,
%   email={turtle@seacoast.wl},
%   address={Institute of Formal and Applied Linguistics\\
%            Faculty of Mathematics and Physics,\\
%            Charles University in Prague\\
%            Malostranské náměstí 25\\
%            118 00 Praha 1, Czech Republic}}
% \author{firstname=Cheshire, surname=Cat}


% The title and authors' names are used in the running head. If they are
% long, you should define short versions. These definitions are optional. You
% define them only if they are needed. The example follows:
\shorttitle{SCFG Decoder Implementations}
\shortauthor{Hoang, Heafield et al}

% Now print the title by:
\PBMLmaketitle


% Abstract
% ========
% The abstract is placed within the "abstract" environment. It is a mandatory
% part of the article. PLEASE, do not use your own macros in abstract, if possible.

\begin{abstract}
Best practise for developing a SMT decoder has progressed enormously since the initial release of the Moses decoder in 2006. The emergence of syntax-inspired models, sparse features and more features, have been the driver for the major changes in the framework of the decoder.

In this paper, we describe the changes in the Moses decoder and supporting programs to support such changes, while preserving the rich features in the Moses toolkit. Particular emphasis will be place in describing the feature function framework.

The open-source community has also changed from the dominance of the phrase-based model and virtual Moses monopoly, to a situation where syntactic models are leading the research agenda, and other projects such as cdec and Joshua have equal prominence.  We take this opportunity to compare the implementations of these other decoders with Moses. We find that there are many similarities between the decoders, but also some surprising differences in implementation of even very fundamental features in each decoder.

\end{abstract}

\section{Introduction}
% The body of the article
% =======================
% The PBML class is modelled after the standard article class. This means
% that you can use almost everything that is allowed in articles as described
% in the textbooks of LaTeX. We support sectioning commands \section,
% \subsection and \subsubsection.

% In addition to the \cite command, you can use natbib style of citations.
% PLEASE, use \citet instead of \cite if the author names are part of the sentence.
%\citet{PDT2} show \ldots \\ % This renders as "Hajič et al. (2006) show" instead of "(Hajič et al., 2006) show".
%\ldots tree-based annotation (e.g.~\citealp{PDT2}).

In terms of extensibility, the Moses\cite{koehn-EtAl:2007:PosterDemo} decoder was a vast improvement over its immediate predecessor, Pharoah\cite{Koehn:Pharaoh:2004}. For instance, Moses allows multiple translation models and language models \emph{instances} and \emph{implementations}. 

Allowing multiple instances has enabled researchers to combine multiple datasets in novel ways such as the use of secondary models with differing linguistic information\cite{Koehn:Philipp:2007, Sennrich:Rico:2011,Koehn:Philipp:2012}. Allowing multiple implementations has spurred on the developments of smaller, faster, better implementation of language models and phrase tables\cite{talbot-osborne:2007:ACLMain, Heafield-kenlm, Junczys-Dowmunt:Marcin:2012}.

However, a direction of machine translation research in the last few years has been on improving translation quality with the addition of ad-hoc feature functions to soft-constrain the search process. The use of sparse features during decoding has also been popular. Research such as \cite{chiang-knight-wang:2009:NAACLHLT09, cherry:2013:NAACL-HLT} typifies this approach.

Enabling ad-hoc features in Moses requires a framework where new feature functions can be easily added. Adding support for sparse features entails fundamental changes in the way features function scores are stored and calculated. It is generally known that MERT\cite{Och:MERT} is unstable with a large number of scores, using sparse features will also require tuning algorithms which are able to deal with a large, non-predetermined number of feature scores. 

This paper describes the changes to the Moses decoder in order to incorporate more feature functions, sparse feature scores, and compares it with that implemented in cdec\cite{Dyer_etal_2010} and Joshua\cite{Joshua-Decoder}.

\section{Feature Function Framework}
\subsection{Old Framework}

The standard feature functions in the phrase-based models are:
\begin{itemize}
  \item Translation model
  \item Language model
  \item Distortion model
  \item Lexicalized reordering model
  \item Word penalty
  \item Unknown word penalty
\end{itemize}
The hierarchical and syntax model also uses these feature functions, except for the distortion and lexicalized reordering model.

Each feature function is implemented as a separate class, with no common class hierarchy. The initialisation and evaluation of each feature function was entirely bespoked.

Feature function state information is an essential component of the search procedure in decoding. State information control hypothesis recombination, enabling the decoder to effectively search through many more hypotheses than it actually has. State information is emitted from stateful feature functions.

The state information in Moses has evolved since its inception. Initially, state information was only available for language models, therefore, only language models states are compared, Figure~\ref{algo:CompareHypoStates}.

\begin{figure} [h!]
\small 
\begin{algorithmic}
\REQUIRE hypothesis $a$, hypothesis b

       \IF {coverage of a != coverage of b }
	\RETURN not equal
       \ENDIF
       
        \FORALL {LM state s}
          \IF {state s of hypothesis a != state s for hypothesis b}
	    \RETURN not equal
	 \ENDIF
        \ENDFOR
        \RETURN equal

\end{algorithmic}
\caption{Comparing Hypothesis States (old framework)}
\label{algo:CompareHypoStates}
\end{figure} 

\subsection{New Framework}        
 All feature functions inherit indirectly from the class \emph{FeatureFunction}, which provides basic services to all feature functions such as storing the name and number of dense features, and providing a central repository of all feature functions.

A feature function can be stateless or stateful, depending on whether it inherits from \emph{StatelessFeatureFunction} or \emph{StatefulFeatureFunction}, respectively.

The state comparison routine of Figure~\ref{algo:CompareHypoStates} was generalised to compare the state of all stateful feature functions, Figure~\ref{algo:CompareHypoStates.new}.

\begin{figure} [h!]
\small 
\begin{algorithmic}
\REQUIRE hypothesis $a$, hypothesis b

       \IF {coverage of a != coverage of b }
	\RETURN not equal
       \ENDIF
       
        \FORALL {feature function state s}
          \IF {state s of hypothesis a != state s for hypothesis b}
	    \RETURN not equal
	 \ENDIF
        \ENDFOR
        \RETURN equal

\end{algorithmic}
\caption{Comparing Hypothesis States (new framework)}
\label{algo:CompareHypoStates.new}
\end{figure} 
        
\subsection{cdec and Joshua}
cdec and Joshua both have modern feature function frameworks similar to the Moses new framework. cdec has no distinction between stateful and stateless features; stateless features have zero size states.

\section{Hypothesis and Translation Rule Evaluation}
\subsection{Old Framework}
The set of feature functions are hardcoded when evaluating the scores of hypotheses and translation rules. Translation rules are evaluated according to Equation~\ref{eq:translation rule score}:
\begin{equation}
  score = \sum_{m \in T} \sum_{i \in m} (\lambda_{m,i} * score_{m,i}) + \sum_{l  \in L} (\lambda_l * score_i) + \lambda_{wp} * words
\label{eq:translation rule score}
\end{equation}
where $T$ is the set of translation models, and $L$ is the set of sequence models, $\lambda_{m,i}$ is the i'th weight for translation model $m$, $\lambda_l$ is the weight for language model $l$, $\lambda_{wp}$ is the word penalty weight.

Similarly, hypotheses are evaluated as follows, Equation~\ref{eq:hypothesis rule score}:
\begin{equation}
\begin{split}
 score & =  \sum_{m \in T} \sum_{i \in m} (\lambda_{m,i} * score_{m,i}) \\
    & + \sum_{l \in L} (\lambda_l * score_l) \\
    & + \lambda_{wp} * words \\
    & + \lambda_d * score_d \\
    & + \sum_{m \in R} \sum_{i \in m} (\lambda_{m,i} * score_{m,i})  \\
    & + score_{future} \\
\label{eq:hypothesis rule score}
\end{split}
\end{equation}
where $\lambda_d$ is the distortion weight, and $R$ is the set of lexicalized reordering models, and $score_{future}$ is the estimate of the score to completion for the hypothesis.

When adding a new feature function, its scores would not be included in the translation rule or hypothesis scores until the implementation of Equation~\ref{eq:translation rule score} and Equation~\ref{eq:hypothesis rule score} are update.

%Translation rule always evaluated occurs before decoding, either during loading of the feature function, or just before the translation rule is used in decoding.

%Hypotheses are evaluated when they are created during the search process.

\subsection{New Framework}
The new framework better supports the creation of arbitrary feature functions by generalizing all code that deals with feature functions.

Equation~\ref{eq:translation rule score} becomes Equation~\ref{eq:translation rule score.new}
\begin{equation}
  score = \sum_{m \in FF} \sum_{i \in m} (\lambda_{m,i} * score_{m,i})
\label{eq:translation rule score.new}
\end{equation}
where $FF$ is the set of all feature functions.

Similarly, Equation~\ref{eq:hypothesis rule score} becomes Equation~\ref{eq:hypothesis rule score.new}:
\begin{equation}
  score =  \sum_{m \in FF} \sum_{i \in m} (\lambda_{m,i} * score_{m,i})
	  + score_{future}
\label{eq:hypothesis rule score.new}
\end{equation}

Furthermore, there are three juncture during decoding where each feature function can be evaluated. At each point, a different range of information is available to the feature function. 
\begin{enumerate}
    \item During creation of translation rules. Has access to the words in the source and target side of translation rule, but not the entire input sentence.
    \item Just before decoding. Has access to the entire source sentence to be decoded.
    \item During decoding. Access to the context information is possible, containing the current and antecedent hypotheses. The exact information contained in the hypotheses is dependent on the formalism used.
\end{enumerate}
However, the stage at which the feature function is evaluate determines whether the feature function is cacheable between sentences. Scores that are evaluated at stage (1) can be cached and reused inter-sentence. Scores evaluated during stage (2) and (3) cannot be cached as they depend on the entire source sentence. Therefore, it is more efficient to evaluate as early as possible.

Additionally, some feature functions do not override any of the stages. These feature is so intertwined with the search operation that their scores are set by search operation themselves. The feature function exist as an identifier to locate the score. Features functions such as the translation models, unknown word penalty,  generation models, and input feature score fall into this category.

\subsection{Examples}

Each feature function can override any or all of the methods to evaluate at each of these stage. For example. a simple word penalty feature would typically implement its evaluation in stage (1) as access to the translation rule is sufficient information to allow it to evaluate the word penalty score.

On the other hand, a feature that depends on the bag of words of the input sentence must override the evaluation method for stage (2).

Stateful features need to override stage (3). Features that depend on the segmentation of the input sentence also need to override this stage. 

The most efficient way to implement language models is to override stage (1) and (3). As a stateful feature, it must override stage (3) to access adjacent target words to score overlapping n-grams, and to set the current state. However, it is able to evaluate the n-grams within each translation rule without the context, therefore, language models should also override stage (1). The calculation of estimate future cost should also occur in stage (1). This behaviour codify existing practises by building the framework to support it, allowing other feature functions to take advantage of this efficient mechanism.

\subsection{cdec and Joshua}
In contrast to Moses which uses a one-pass search strategy, cdec and Joshua uses a multi-pass search strategy. That is, for each input sentence, all feature functions are evaluated when a hypothesis is created in Moses.

In cdec and Joshua, each pass creates a hypergraph that is used as the input for the next pass. Both decoders evaluates stateless feature functions in the first pass, followed by stateful feature functions in the second. Neither decoder partially evaluate stateful feature functions to estimate future cost, as done in Moses.

cdec also supports an arbitrary number of passes, feature functions can be assigned to a particular pass.

\section{File Format}
\subsection{Old Framework}
Each feature function also required a separate section in the configuration (moses.ini) file to be created, along with the code to read the new section. For example, translation models are specified in the section
\begin{verbatim}
     [ttable-file]
\end{verbatim}
Language models are in
\begin{verbatim}
     [lmodel-file]
\end{verbatim}
Similarly, each feature function also need a separate weight section, for example, for translation models:
\begin{verbatim}
     [weight-t]
\end{verbatim}
and language models:
\begin{verbatim}
     [weight-l]
\end{verbatim}
New feature functions need to create and manage their own specification and weight sections.

The tuning script must also be informed of new weight sections by manually editing the MERT script.

\subsection{New Framework}
The configuration file format has been changed to minimize the amount of code changes necessary when adding new feature functions. All feature functions are declared in the configuration file in the section
\begin{verbatim}
  [feature]
\end{verbatim}
In the fixed format
\begin{verbatim}
  Feature-Name [key=value]*
\end{verbatim}
Similarly, all dense \emph{weights} are put into the section
\begin{verbatim}
  [weight]
\end{verbatim}
In the fixed format
\begin{verbatim}
  Feature-Instance= [value]*
\end{verbatim}
Weights for sparse features are in separate file referenced by the section
\begin{verbatim}
  [weight-file]
\end{verbatim}
in the format
\begin{verbatim}
  Feature-Instance feature value
\end{verbatim}
The following is an example section from a vanilla phrase-based configuration file:
\begin{verbatim}
  [feature]
  UnknownWordPenalty
  WordPenalty
  PhraseDictionaryMemory name=TranslationModel0 table-limit=20 num-features=5...
  LexicalReordering name=LexicalReordering0 num-features=6 type=wbe-msd...
  Distortion
  KENLM lazyken=1 name=LM0 factor=0 path=europarl.binlm.1 order=5
\end{verbatim}
Notice that feature functions \emph{UnknownWordPenalty}, \emph{WordPenalty}, and \emph{Distortion} must be explicitly specified. In the old format, these features were implicitly included for all phrase-based models.

The weights section from the same moses.ini file is shown below:
\begin{verbatim}
  [weight]
  UnknownWordPenalty0= 1
  WordPenalty0= -1
  TranslationModel0= 0.2 0.2 0.2 0.2 0.2
  LexicalReordering0= 0.3 0.3 0.3 0.3 0.3 0.3
  Distortion0= 0.3
  LM0= 0.5
\end{verbatim}


\section{Sparse Feature Function}

Sparse features contain scores that may only be present occasionally. However, whereas there are typically 14 scores from five feature functions in a standard phrase-based model and 7 scores in a hierarchical phrase-based model, there may be millions of sparse scores in a system with sparse features. Only a few will be present in a particular hypothesis or translation rule.

Storing and using sparse feature efficiently present a challenge which each decoder has attempted to solve differently.

\subsection{Moses}
Moses implements supports for both dense and sparse features. Each score must be identifiable as one or the other.
The intuition behind this is that most systems will still contain dense features. Therefore, it would be more efficient to use existing dedicated data-structures to efficiently handle dense vectors.

A sparse feature map was added to handle sparse scores.

\subsection{cdec}
cdec eschew Moses' dense and sparse hybrid approach and implement only sparse maps. Dense features can still be modelled in the framework as sparse features, whose name are suffixed with indices.

To improve speed and memory usage, hypotheses only store deltas of feature differences with antecedent hypotheses.

\subsection{Joshua}
As with cdec, Joshua only supports sparse features, however, it uses a different strategy to improve efficiency.

Hypotheses only retain the weighted sum of the feature scores, . The individual score components are not stored unless required, for example, in the n-best list. The intuition behind this approach is that the score breakdown is only required in some situations. In most other circumstances, they can be traded for better speed and lower memory consumption.
  
\section{Results}

The results from the three decoders are comparable, given the same formalism and model data, and adjusting for scaling differences and constants. . Figure~\ref{fig:model} shows the comparison of model scores and BLEU scores for a hierarchical model. Moses and cdec are roughly comparable in terms of speed. Also, Moses and cdec converges to roughly the same model scores, while Joshua makes more search errors, leading to lower BLEU scores.

\begin{figure}
%run report.gnuplot to update these.
\input{1111-8/model.tex}\input{1111-8/bleu.tex}
\caption{\label{fig:model}Model scores and BLEU using cube pruning with pop limits ranging from 5 to 1000.}
\end{figure} 

Joshua's lower model score is likely due to the fact that in Moses and cdec, the calculation of language model state is delegated to the language model implementation~\cite{Heafield-left} which uses the internal data-structure of the language model to determine state. Joshua, on the other hand, uses the words in the left and right context to determine language model state. This results in more aggressive hypothesis recombination in Moses and cdec, leading to better model score. Joshua will soon be updated~\footnote{\url{https://github.com/hieuhoang/papers/commit/e417a6adcf4f1e49954836b14799509fecdea11c#mt-marathon.2013} } to incorporate better left-state minimisation to match the performance of Moses and cdec. 

\subsection{Glue Rules}

The glue grammar can be constructed in a number of ways. In formulation (1), the grammar is:
 \begin{verbatim}
    S -> <s>
    S -> S X
    GOAL -> S </s> 
 \end{verbatim}
 where <s> and </s> are first-class words like any other. In this formulation, glued hypotheses know that they are bound to the beginning of sentence <s>.  Therefore, their left language model state is empty and more hypotheses can recombine.  
 
The grammar for formulation (2) is:
 \begin{verbatim}
    S -> X
    S -> S X
    GOAL -> <s> S </s> 
 \end{verbatim}
Glued hypotheses are informed about <s> and </s> only after forming a complete sentence, so the left states are spuriously ambiguous.  Moreover, hypothesis score estimates are more accurate in (1) as they know about <s> earlier. 

Figure~\ref{fig:glue} shows the improvement in cdec by changing the glue rule formulation from (2) to (1). The glue grammar for each decoder had varied since their inceptions. The convergence on using formulation (1) is the direct result of the comparison between the three decoders.

 \begin{figure}[h]
 \begin{center}
 \input{1111-8/glue_model.tex}
 \end{center}
 \caption{Affect on model score by changing glue-rule formulation}
 \label{fig:glue}
 \end{figure}


\section{Conclusion}

Statistical machine translation is a dynamic field which is constantly evolving. A major challenge for the Moses project, in common with many long-term software projects, is to maintain and update the the underlying framework and infrastructure to enable this evolution to continue. 


\section*{Acknowledgements}

The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement 288487 (MosesCore).

% Bibliography
% ============
% You may either enter the bibliography manually, possibly making use of
% \label and \ref, or use BibTeX. The bibliography style is set
% automatically. You process the bibliography by BibTeX in the
% standard way and include it by:
\bibliography{mybib}

% If needed, add appendices here
%\section*{Appendix A: \ldots}

\correspondingaddress
\end{document}

% ======= Additional information ==========

% XeLaTeX
% =======
% The Prague Bulletin of Mathematical Linguistics is typeset by XeLaTeX.
% It is not required that you use XeLaTeX and the same fonts as will be used in the
% journal but it is better to do so if you could (so you have the same line and page breaks).
% If XeLaTeX is not available on your computer, you can even used standard LaTeX.

% Fonts
% =====
% In the printed version, fonts TeX Gyre Pagella (shipped with TeX Live 2008 or later)
% and DejaVu (http://dejavu.sourceforge.net/wiki/index.php/Main_Page)
% will be used. If you have XeLaTeX but not these fonts, you may need to install
% the fonts somewhere where your system (fontconfig) can find them. On Linux,
% try something like this:
% ln -s YOUR-TEXLIVE/2008/texmf-dist/fonts/opentype/public/ \
%  ${HOME}/.fonts/texlive-2008-otf-public
% If you are not able to install these fonts, you can instruct XeLaTeX
% to use its default fonts by \document[nofonts]{pbml}.

% If you want to typset examples in east Asian scripts, you have to use
% OpenType Unicode fonts that are freely redistributable and you have to
% include them with your article. If you must use nonfree or non-Unicode
% fonts, you must supply the examples as EPS or PDF with fonts embedded in
% the files (as the required subset).
% If encountering problems with the "bidi" package, contact pbml@ufal.mff.cuni.cz.

% Vector Graphics
% ===============
% The most portable graphics package is tikz which is a front end to pgf.
% This package is fully supported both in standard LaTeX and XeLaTeX.
% PSTricks is not currently available for XeLaTeX.

% Color Images
% ============
% The printed version of PBML does not allow color figures, so please convert
% all your figures to grayscale (or black&white), make sure they are legible
% and put them in a subdirectory called "grayscale".
% When specifying the filename in \includegraphics DO NOT include the
% subdirectory.
% Optionally, in addition to the default grayscale images, you can add also
% color versions of the figures to be published in the online version.
% Put such images in a subdirectory called "color" with the same filenames
% as the grayscale ones.
% To temporarily switch to color output and hypertext links, use
% \documentclass[color]{pbml}

% Including images and other files
% ===================================
% When loading the file you must use a relative path, never the full path.
% Load the images by \includegraphics
% and other files by \input, never use \include.
